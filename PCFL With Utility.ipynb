{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1423)\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# import sys\n",
    "# import matplotlib\n",
    "# %matplotlib inline\n",
    "# from matplotlib import pyplot as plt\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# import theano\n",
    "# import lasagne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_spread_IFT(filename):\n",
    "    filename=filename[['TIME_PERIOD', 'OBS_VALUE']]\n",
    "    filename['Year'] = filename['TIME_PERIOD'].str.split('-').str[0]\n",
    "\n",
    "    filename = filename.rename(columns={'OBS_VALUE': 'UNRATE','TIME_PERIOD': 'DATE'})\n",
    "    filename['DATE'] = pd.to_datetime(filename['DATE'])\n",
    "    filename = filename[(filename['DATE'].dt.year >= 1994) & (filename['DATE'].dt.year <= 2021)]\n",
    "    filename['Year'] = filename['DATE'].dt.year\n",
    "    filename['Quarter'] = filename['DATE'].dt.quarter\n",
    "    filename = filename.groupby(['Year', 'Quarter'])['UNRATE'].mean().reset_index()\n",
    "    filename['DATE'] = filename['Year'].astype(str) + '-Q' + filename['Quarter'].astype(str)\n",
    "    return  filename[['DATE', 'UNRATE']]\n",
    "\n",
    "\n",
    "def data_Spread_Unrate(file_name):\n",
    "    file_name = file_name.rename(columns={' Unemployment Rate (%)': 'UNRATE','Year': 'DATE'})\n",
    "    file_name['DATE'] = file_name['DATE'].astype(int)\n",
    "    file_name = file_name[(file_name['DATE'] >= 1994) & (file_name['DATE'] <= 2021)]\n",
    "    file_name['Year'] = file_name['DATE']\n",
    "\n",
    "\n",
    "    years = list(range(1994, 2022))\n",
    "    quarters = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "\n",
    "    date_list = []\n",
    "    for year in years:\n",
    "        for quarter in quarters:\n",
    "            date_list.append(str(year) + quarter)\n",
    "\n",
    "    quarters_df = pd.DataFrame(date_list, columns=['DATE'])\n",
    "\n",
    "    quarters_df['Year'] = quarters_df['DATE'].apply(lambda x: int(x[:4]))\n",
    "    quarters_df['Quarter'] = quarters_df['DATE'].apply(lambda x: x[4:])\n",
    "\n",
    "    file_name = pd.merge(quarters_df, file_name, on='Year', how='left')\n",
    "    file_name['DATE'] = file_name['Year'].astype(str) + '-' + file_name['Quarter'].astype(str)\n",
    "    return file_name[['DATE','UNRATE']]\n",
    "\n",
    "\n",
    "def data_Spread_GDP(file_name):\n",
    "\n",
    "    file_name = file_name.rename(columns={'Year': 'DATE'})\n",
    "    file_name['DATE'] = file_name['DATE'].astype(int)\n",
    "    file_name = file_name[(file_name['DATE'] >= 1994) & (file_name['DATE'] <= 2021)]\n",
    "    file_name['Year'] = file_name['DATE']\n",
    "\n",
    "\n",
    "    years = list(range(1994, 2022))\n",
    "    quarters = ['Q1', 'Q2', 'Q3', 'Q4']\n",
    "\n",
    "    date_list = []\n",
    "    for year in years:\n",
    "        for quarter in quarters:\n",
    "            date_list.append(str(year) + quarter)\n",
    "\n",
    "    quarters_df = pd.DataFrame(date_list, columns=['DATE'])\n",
    "\n",
    "    quarters_df['Year'] = quarters_df['DATE'].apply(lambda x: int(x[:4]))\n",
    "    quarters_df['Quarter'] = quarters_df['DATE'].apply(lambda x: x[4:])\n",
    "\n",
    "    file_name = pd.merge(quarters_df, file_name, on='Year', how='left')\n",
    "    file_name['DATE'] = file_name['Year'].astype(str) + '-' + file_name['Quarter'].astype(str)\n",
    "    return file_name[['DATE','GDP']]\n",
    "\n",
    "def clean_duplicate(filename):\n",
    "    start_time = '1994-Q1'\n",
    "    end_time = '2021-Q4'\n",
    "    filename = filename[(filename['TIME'] >= start_time) & (filename['TIME'] <= end_time)]\n",
    "    filename = filename.reset_index(drop=True)\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20248\\AppData\\Local\\Temp\\ipykernel_284\\3493353210.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filename['Year'] = filename['TIME_PERIOD'].str.split('-').str[0]\n",
      "C:\\Users\\20248\\AppData\\Local\\Temp\\ipykernel_284\\3493353210.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filename['Year'] = filename['TIME_PERIOD'].str.split('-').str[0]\n",
      "C:\\Users\\20248\\AppData\\Local\\Temp\\ipykernel_284\\3493353210.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filename['Year'] = filename['TIME_PERIOD'].str.split('-').str[0]\n"
     ]
    }
   ],
   "source": [
    "une_USA=pd.read_csv('une_USA.csv')\n",
    "une_USA['DATE'] = pd.to_datetime(une_USA['DATE'])\n",
    "une_USA = une_USA[(une_USA['DATE'].dt.year >= 1994) & (une_USA['DATE'].dt.year <= 2021)]\n",
    "une_USA['Year'] = une_USA['DATE'].dt.year\n",
    "une_USA['Quarter'] = une_USA['DATE'].dt.quarter\n",
    "une_USA = une_USA.groupby(['Year', 'Quarter'])['UNRATE'].mean().reset_index()\n",
    "une_USA['DATE'] = une_USA['Year'].astype(str) + '-Q' + une_USA['Quarter'].astype(str)\n",
    "une_USA = une_USA[['DATE', 'UNRATE']]\n",
    "\n",
    "\n",
    "une_EU=pd.read_csv('une_EU.csv')\n",
    "\n",
    "une_FRA = une_EU[une_EU['geo'] == 'FR']\n",
    "une_DEU = une_EU[une_EU['geo'] == 'DE']\n",
    "une_DNK = une_EU[une_EU['geo'] == 'DK']\n",
    "\n",
    "\n",
    "une_FRA = data_spread_IFT(une_FRA)\n",
    "une_DEU = data_spread_IFT(une_DEU)\n",
    "une_DNK = data_spread_IFT(une_DNK)\n",
    "# une_EU=une_EU[['TIME_PERIOD', 'OBS_VALUE']]\n",
    "# une_EU['Year'] = une_EU['TIME_PERIOD'].str.split('-').str[0]\n",
    "# une_EU = une_EU.rename(columns={'OBS_VALUE': 'UNRATE','TIME_PERIOD': 'DATE'})\n",
    "# une_EU['DATE'] = pd.to_datetime(une_EU['DATE'])\n",
    "# une_EU = une_EU[(une_EU['DATE'].dt.year >= 1994) & (une_EU['DATE'].dt.year <= 2021)]\n",
    "# une_EU['Year'] = une_EU['DATE'].dt.year\n",
    "# une_EU['Quarter'] = une_EU['DATE'].dt.quarter\n",
    "# une_EU = une_EU.groupby(['Year', 'Quarter'])['UNRATE'].mean().reset_index()\n",
    "# une_EU['DATE'] = une_EU['Year'].astype(str) + '-Q' + une_EU['Quarter'].astype(str)\n",
    "# une_EU = une_EU[['DATE', 'UNRATE']]\n",
    "\n",
    "\n",
    "une_CN=pd.read_csv('china-unemployment-rate.csv')\n",
    "une_CN = une_CN.drop(range(0, 15))\n",
    "une_CN = une_CN.drop(une_CN.columns[3], axis=1)\n",
    "une_CN.columns = une_CN.iloc[0]\n",
    "une_CN = une_CN.drop(une_CN.index[0])\n",
    "une_CN.reset_index(drop=True, inplace=True)\n",
    "une_CN['Year'] = une_CN['date'].str.split('/').str[0]\n",
    "une_CN = une_CN.drop('date', axis=1)\n",
    "une_CN = une_CN[[ 'Year', ' Unemployment Rate (%)']]\n",
    "une_CN = data_Spread_Unrate(une_CN)\n",
    "\n",
    "\n",
    "IFT=pd.read_csv('IFT_CNEUUSA.csv')\n",
    "\n",
    "IFT_CN = IFT[IFT['LOCATION'] == 'CHN'][[ 'TIME', 'Value']]\n",
    "IFT_CN = clean_duplicate(IFT_CN)\n",
    "IFT_DNK = IFT[IFT['LOCATION'] == 'DNK'][[ 'TIME', 'Value']]\n",
    "IFT_DNK = clean_duplicate(IFT_DNK)\n",
    "IFT_FRA = IFT[IFT['LOCATION'] == 'FRA'][[ 'TIME', 'Value']]\n",
    "IFT_FRA = clean_duplicate(IFT_FRA)\n",
    "IFT_DEU = IFT[IFT['LOCATION'] == 'DEU'][[ 'TIME', 'Value']]\n",
    "IFT_DEU = clean_duplicate(IFT_DEU)\n",
    "IFT_USA = IFT[IFT['LOCATION'] == 'USA'][[ 'TIME', 'Value']]\n",
    "IFT_USA = clean_duplicate(IFT_USA)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "GDP = pd.read_csv('GDP2.csv')\n",
    "GDP_CN = pd.DataFrame(GDP['40'].loc[4:])\n",
    "GDP_CN = GDP_CN.rename(columns={GDP_CN.columns[0]: 'GDP'})\n",
    "GDP_CN.insert(0, 'Year', range(1960, 2022))\n",
    "\n",
    "GDP_DEU\t = pd.DataFrame(GDP['55'].loc[4:])\n",
    "GDP_DEU\t= GDP_DEU.rename(columns={GDP_DEU\t.columns[0]: 'GDP'})\n",
    "GDP_DEU\t.insert(0, 'Year', range(1960, 2022))\n",
    "\n",
    "GDP_FRA = pd.DataFrame(GDP['77'].loc[4:])\n",
    "GDP_FRA= GDP_FRA.rename(columns={GDP_FRA.columns[0]: 'GDP'})\n",
    "GDP_FRA.insert(0, 'Year', range(1960, 2022))\n",
    "\n",
    "GDP_DNK = pd.DataFrame(GDP['58'].loc[4:])\n",
    "GDP_DNK= GDP_DNK.rename(columns={GDP_DNK.columns[0]: 'GDP'})\n",
    "GDP_DNK.insert(0, 'Year', range(1960, 2022))\n",
    "\n",
    "GDP_USA = pd.DataFrame(GDP['251'].loc[4:])\n",
    "GDP_USA= GDP_USA.rename(columns={GDP_USA.columns[0]: 'GDP'})\n",
    "GDP_USA.insert(0, 'Year', range(1960, 2022))\n",
    "\n",
    "GDP_CN = data_Spread_GDP(GDP_CN)\n",
    "GDP_DEU = data_Spread_GDP(GDP_DEU)\n",
    "GDP_FRA = data_Spread_GDP(GDP_FRA)\n",
    "GDP_DNK = data_Spread_GDP(GDP_DNK)\n",
    "GDP_USA = data_Spread_GDP(GDP_USA)\n",
    "\n",
    "CN_data = pd.concat([GDP_CN, IFT_CN['Value'], une_CN['UNRATE']], axis=1)\n",
    "USA_data = pd.concat([GDP_USA, IFT_USA['Value'], une_USA['UNRATE']], axis=1)\n",
    "DEU_data = pd.concat([GDP_DEU, IFT_DEU['Value'], une_DEU['UNRATE']], axis=1)\n",
    "DNK_data = pd.concat([GDP_DNK, IFT_DNK['Value'], une_DNK['UNRATE']], axis=1)\n",
    "FRA_data = pd.concat([GDP_FRA, IFT_FRA['Value'], une_FRA['UNRATE']], axis=1)\n",
    "\n",
    "CN_data['GDP'] = pd.to_numeric(CN_data['GDP'], errors='coerce')\n",
    "CN_data['GDP'] = CN_data['GDP']  / 1e8\n",
    "\n",
    "\n",
    "USA_data['GDP'] = pd.to_numeric(USA_data['GDP'], errors='coerce')\n",
    "USA_data['GDP'] = USA_data['GDP']  / 1e8\n",
    "\n",
    "DEU_data['GDP'] = pd.to_numeric(DEU_data['GDP'], errors='coerce')\n",
    "DEU_data['GDP'] = DEU_data['GDP']  / 1e8\n",
    "\n",
    "DNK_data['GDP'] = pd.to_numeric(DNK_data['GDP'], errors='coerce')\n",
    "DNK_data['GDP'] = DNK_data['GDP']  / 1e8\n",
    "\n",
    "FRA_data['GDP'] = pd.to_numeric(FRA_data['GDP'], errors='coerce')\n",
    "FRA_data['GDP'] = FRA_data['GDP']  / 1e8\n",
    "\n",
    "\n",
    "ift_cn = np.array(CN_data['Value']).reshape(-1,1)\n",
    "une_cn = np.array(CN_data['UNRATE']).reshape(-1,1)\n",
    "data_cn = np.hstack((ift_cn,une_cn))\n",
    "gdp_cn = np.array(CN_data['GDP']).reshape(-1,1)\n",
    "gdp_cn =  pd.DataFrame(gdp_cn)\n",
    "data_cn =  pd.DataFrame(data_cn)\n",
    "\n",
    "ift_usa = np.array(USA_data['Value']).reshape(-1,1)\n",
    "une_usa = np.array(USA_data['UNRATE']).reshape(-1,1)\n",
    "data_usa = np.hstack((ift_usa,une_usa))\n",
    "gdp_usa = np.array(USA_data['GDP']).reshape(-1,1)\n",
    "gdp_usa =  pd.DataFrame(gdp_usa)\n",
    "data_usa =  pd.DataFrame(data_usa)\n",
    "\n",
    "ift_dnk = np.array(DNK_data['Value']).reshape(-1,1)\n",
    "une_dnk = np.array(DNK_data['UNRATE']).reshape(-1,1)\n",
    "data_dnk = np.hstack((ift_dnk,une_dnk))\n",
    "gdp_dnk = np.array(DNK_data['GDP']).reshape(-1,1)\n",
    "gdp_dnk =  pd.DataFrame(gdp_dnk)\n",
    "data_dnk =  pd.DataFrame(data_dnk)\n",
    "\n",
    "ift_fra = np.array(FRA_data['Value']).reshape(-1,1)\n",
    "une_fra = np.array(FRA_data['UNRATE']).reshape(-1,1)\n",
    "data_fra = np.hstack((ift_fra,une_fra))\n",
    "gdp_fra = np.array(FRA_data['GDP']).reshape(-1,1)\n",
    "gdp_fra =  pd.DataFrame(gdp_fra)\n",
    "data_fra =  pd.DataFrame(data_fra)\n",
    "\n",
    "ift_deu = np.array(DEU_data['Value']).reshape(-1,1)\n",
    "une_deu = np.array(DEU_data['UNRATE']).reshape(-1,1)\n",
    "data_deu = np.hstack((ift_deu,une_deu))\n",
    "gdp_deu = np.array(DEU_data['GDP']).reshape(-1,1)\n",
    "gdp_deu =  pd.DataFrame(gdp_deu)\n",
    "data_deu =  pd.DataFrame(data_deu)\n",
    "\n",
    "merged_df = pd.concat([gdp_cn, gdp_usa], axis=0, ignore_index=True)\n",
    "merged_df2 = pd.concat([merged_df, gdp_dnk], axis=0, ignore_index=True)\n",
    "merged_df3 = pd.concat([merged_df2, gdp_fra], axis=0, ignore_index=True)\n",
    "gdp_total = pd.concat([merged_df3, gdp_deu], axis=0, ignore_index=True)\n",
    "\n",
    "merged_dfa = pd.concat([data_cn, data_usa], axis=0, ignore_index=True)\n",
    "merged_dfb = pd.concat([merged_dfa, data_dnk], axis=0, ignore_index=True)\n",
    "merged_dfc = pd.concat([merged_dfb, data_fra], axis=0, ignore_index=True)\n",
    "data_total = pd.concat([merged_dfc, data_deu], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CN_data['UNRATE'] = pd.to_numeric(CN_data['UNRATE'], errors='coerce')\n",
    "# CN_data['Value'] = pd.to_numeric(CN_data['Value'], errors='coerce')\n",
    "\n",
    "# CN_data['country'] = 'cn'\n",
    "# DNK_data['country'] = 'dnk'\n",
    "# USA_data['country'] = 'usa'\n",
    "# DEU_data['country'] = 'deu'\n",
    "# FRA_data['country'] = 'fra'\n",
    "\n",
    "\n",
    "# ift_cn = np.array(CN_data['Value']).reshape(-1,1)\n",
    "# une_cn = np.array(CN_data['UNRATE']).reshape(-1,1)\n",
    "# data_cn = np.hstack((ift_cn,une_cn))\n",
    "# gdp_cn = np.array(CN_data['GDP']).reshape(-1,1)\n",
    "# gdp_cn =  pd.DataFrame(gdp_cn)\n",
    "# data_cn =  pd.DataFrame(data_cn)\n",
    "\n",
    "# ift_usa = np.array(USA_data['Value']).reshape(-1,1)\n",
    "# une_usa = np.array(USA_data['UNRATE']).reshape(-1,1)\n",
    "# data_usa = np.hstack((ift_usa,une_usa))\n",
    "# gdp_usa = np.array(USA_data['GDP']).reshape(-1,1)\n",
    "# gdp_usa =  pd.DataFrame(gdp_usa)\n",
    "# data_usa =  pd.DataFrame(data_usa)\n",
    "\n",
    "# ift_dnk = np.array(DNK_data['Value']).reshape(-1,1)\n",
    "# une_dnk = np.array(DNK_data['UNRATE']).reshape(-1,1)\n",
    "# data_dnk = np.hstack((ift_dnk,une_dnk))\n",
    "# gdp_dnk = np.array(DNK_data['GDP']).reshape(-1,1)\n",
    "# gdp_dnk =  pd.DataFrame(gdp_dnk)\n",
    "# data_dnk =  pd.DataFrame(data_dnk)\n",
    "\n",
    "# ift_fra = np.array(FRA_data['Value']).reshape(-1,1)\n",
    "# une_fra = np.array(FRA_data['UNRATE']).reshape(-1,1)\n",
    "# data_fra = np.hstack((ift_fra,une_fra))\n",
    "# gdp_fra = np.array(FRA_data['GDP']).reshape(-1,1)\n",
    "# gdp_fra =  pd.DataFrame(gdp_fra)\n",
    "# data_fra =  pd.DataFrame(data_fra)\n",
    "\n",
    "# ift_deu = np.array(DEU_data['Value']).reshape(-1,1)\n",
    "# une_deu = np.array(DEU_data['UNRATE']).reshape(-1,1)\n",
    "# data_deu = np.hstack((ift_deu,une_deu))\n",
    "# gdp_deu = np.array(DEU_data['GDP']).reshape(-1,1)\n",
    "# gdp_deu =  pd.DataFrame(gdp_deu)\n",
    "# data_deu =  pd.DataFrame(data_deu)\n",
    "\n",
    "# merged_df = pd.concat([gdp_cn, gdp_usa], axis=0, ignore_index=True)\n",
    "# merged_df2 = pd.concat([merged_df, gdp_dnk], axis=0, ignore_index=True)\n",
    "# merged_df3 = pd.concat([merged_df2, gdp_fra], axis=0, ignore_index=True)\n",
    "# gdp_total = pd.concat([merged_df3, gdp_deu], axis=0, ignore_index=True)\n",
    "\n",
    "# merged_df = pd.concat([data_cn, data_usa], axis=0, ignore_index=True)\n",
    "# merged_df2 = pd.concat([merged_df, data_dnk], axis=0, ignore_index=True)\n",
    "# merged_df3 = pd.concat([merged_df2, data_fra], axis=0, ignore_index=True)\n",
    "# data_total = pd.concat([merged_df3, data_deu], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CN_data.to_csv('CN_data.csv')\n",
    "def u(inflation, unrate,country):\n",
    "    if country == 'cn':\n",
    "        return  10261 * 0.762**(1 - float(unrate)) *(1 + float(inflation))\n",
    "    if country == 'usa':\n",
    "        return  65280 * 0.612**(1 - float(unrate)) *(1 + float(inflation))\n",
    "    if country == 'fra':\n",
    "        return  44670 * 0.645**(1 - float(unrate)) *(1 + float(inflation))\n",
    "    if country == 'dnk':\n",
    "        return  60170 * 0.768**(1 - float(unrate)) *(1 + float(inflation))\n",
    "    if country == 'deu':\n",
    "        return  50206 * 0.639**(1 - float(unrate)) *(1 + float(inflation))\n",
    "        \n",
    "Z_cn = np.array([u(ift_cn[i],une_cn[i],'cn') for i in range(0,112)]).reshape(-1,1)\n",
    "Z_cn = pd.DataFrame(Z_cn)\n",
    "\n",
    "Z_usa = np.array([u(ift_usa[i],une_usa[i],'usa') for i in range(0,112)]).reshape(-1,1)\n",
    "Z_usa = pd.DataFrame(Z_usa)\n",
    "\n",
    "Z_fra = np.array([u(ift_fra[i],une_fra[i],'fra') for i in range(0,112)]).reshape(-1,1)\n",
    "Z_fra = pd.DataFrame(Z_fra)\n",
    "\n",
    "Z_dnk = np.array([u(ift_dnk[i],une_dnk[i],'dnk') for i in range(0,112)]).reshape(-1,1)\n",
    "Z_dnk = pd.DataFrame(Z_dnk)\n",
    "\n",
    "Z_deu = np.array([u(ift_deu[i],une_deu[i],'deu') for i in range(0,112)]).reshape(-1,1)\n",
    "Z_deu = pd.DataFrame(Z_deu)\n",
    "\n",
    "merged_df11 = pd.concat([Z_cn, Z_usa], axis=0, ignore_index=True)\n",
    "merged_df22 = pd.concat([merged_df11, Z_dnk], axis=0, ignore_index=True)\n",
    "merged_df33 = pd.concat([merged_df22, Z_fra], axis=0, ignore_index=True)\n",
    "Z_total = pd.concat([merged_df33, Z_deu], axis=0, ignore_index=True)\n",
    "Z_total = np.array(Z_total)\n",
    "# Z_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regress X on Y using cubic regression. \n",
    "poly = PolynomialFeatures(degree=3)\n",
    "Z_total_poly = poly.fit_transform(Z_total)\n",
    "\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "model = model.fit(Z_total_poly, gdp_total)\n",
    "\n",
    "X_estimator_prag = np.array([model.coef_[0][0] \n",
    "                            + model.coef_[0][1]*Z_total[i] \n",
    "                            + model.coef_[0][2]*Z_total[i]**2 \n",
    "                            + model.coef_[0][3]*Z_total[i]**3 for i in range(0,112)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster Xs.\n",
    "N_CLASSES = 3\n",
    "X_lbls_prag = KMeans(n_clusters=N_CLASSES, n_init=10,).fit_predict(X_estimator_prag.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Clustering P(y | X_lbls).\n"
     ]
    }
   ],
   "source": [
    "#Cluster Ys.\n",
    "m = 112\n",
    "\n",
    "Y_ftrs = np.zeros((une_cn.shape[0],m))\n",
    "for i in range(0,m):\n",
    "        helper_matrix = np.hstack((data_total,Z_total))\n",
    "        helper_matrix = np.delete(helper_matrix,np.where(helper_matrix[:,1] != une_cn[i]),axis=0)\n",
    "        Y_ftrs[i] = helper_matrix[:,2].mean()\n",
    "print('Done. Clustering P(y | X_lbls).')\n",
    "Y_lbls_prag = KMeans(n_clusters=N_CLASSES, n_init=10).fit_predict(Y_ftrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cite the code from Causal Feature Learning for Utility-Maximizing Agents to do the following computation\n",
    "#Create labels showing how variables have been coarsened.\n",
    "\n",
    "full_matrix=np.hstack((data_cn,X_lbls_prag.reshape(-1,1),Y_lbls_prag.reshape(-1,1)))\n",
    "\n",
    "crs_C0 = np.delete(full_matrix,np.where(full_matrix[:,2]!=0),axis=0)\n",
    "crs_C0 = crs_C0[:,0].reshape(-1,1)\n",
    "crs_C0 = np.array([crs_C0[i] for i in range(0,len(crs_C0))])\n",
    "crs_C0 = list(np.unique(crs_C0))\n",
    "crs_C1 = np.delete(full_matrix,np.where(full_matrix[:,2]!=1),axis=0)\n",
    "crs_C1 = crs_C1[:,0].reshape(-1,1)\n",
    "crs_C1 = np.array([crs_C1[i] for i in range(0,len(crs_C1))])\n",
    "crs_C1 = list(np.unique(crs_C1))\n",
    "crs_C2 = np.delete(full_matrix,np.where(full_matrix[:,2]!=2),axis=0)\n",
    "crs_C2 = crs_C2[:,0].reshape(-1,1)\n",
    "crs_C2 = np.array([crs_C2[i] for i in range(0,len(crs_C2))])\n",
    "crs_C2 = list(np.unique(crs_C2))\n",
    "\n",
    "\n",
    "crs_E0 = np.delete(full_matrix,np.where(full_matrix[:,3]!=0),axis=0)\n",
    "crs_E0 = crs_E0[:,1].reshape(-1,1)\n",
    "crs_E0 = np.array([crs_E0[i] for i in range(0,len(crs_E0))])\n",
    "crs_E0 = list(np.unique(crs_E0))\n",
    "crs_E1 = np.delete(full_matrix,np.where(full_matrix[:,3]!=1),axis=0)\n",
    "crs_E1 = crs_E1[:,1].reshape(-1,1)\n",
    "crs_E1 = np.array([crs_E1[i] for i in range(0,len(crs_E1))])\n",
    "crs_E1 = list(np.unique(crs_E1))\n",
    "crs_E2 = np.delete(full_matrix,np.where(full_matrix[:,3]!=2),axis=0)\n",
    "crs_E2 = crs_E2[:,1].reshape(-1,1)\n",
    "crs_E2 = np.array([crs_E2[i] for i in range(0,len(crs_E2))])\n",
    "crs_E2 = list(np.unique(crs_E2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['    ' 'crs_E0' 'crs_E1' 'crs_E2']\n",
      " ['crs_C0' '0.341' '0.0' '0.659']\n",
      " ['crs_C1' '0.0' '1.0' '0.0']\n",
      " ['crs_C2' '0.905' '0.095' '0.0']]\n",
      "crs_C0= [-2.16709, -1.531023, -1.433831, -1.433728, -1.269247, -1.163939, -1.100066, -0.9913543, -0.8645086, -0.8327988, -0.7576911, -0.625702, -0.5931718, -0.5868979, -0.1337218, -0.1089662, 0.09692045, 0.1008965, 0.120048, 0.2640063, 0.2989442, 0.4696215, 0.5267693, 0.6094389, 0.66374, 0.7926206, 0.8069077, 0.9300591, 0.9993294, 1.000762, 1.147486, 1.268126, 1.307824, 1.357313, 1.379565, 1.40732, 1.414098, 1.437909, 1.450253, 1.508197, 1.567239, 1.592239, 1.602879, 1.628989, 1.658429, 1.679921, 1.711886, 1.782459, 1.79357, 1.822323, 1.829653, 1.89781, 1.905997, 1.932727, 1.958384, 2.071556, 2.102102, 2.103176, 2.10914, 2.126289, 2.128351, 2.138235, 2.160904, 2.173218, 2.24056, 2.285898, 2.390944, 2.463619, 2.580492, 2.62852, 2.667514, 2.718078, 2.72193, 2.733513, 2.745838, 2.747699, 2.763158, 2.771089, 2.864337, 2.894226, 2.933743, 3.089114, 3.264564, 3.608638, 5.160919]\n",
      "crs_C1= [19.72486, 21.86715, 22.23648, 22.5835, 25.72584, 26.88372]\n",
      "crs_C2= [3.727297, 4.222709, 4.31882, 4.620924, 4.642333, 4.956629, 5.164686, 5.223647, 5.313106, 5.954095, 6.189586, 6.438447, 6.717938, 6.966722, 7.863462, 7.9266, 8.09657, 9.066221, 9.368176, 11.12624, 14.75736]\n",
      "crs_E0= ['3.12', '4.35', '4.49', '4.52', '4.53', '4.55', '4.57', '4.59', '5']\n",
      "crs_E1= ['2.9', '3']\n",
      "crs_E2= ['3.23', '3.24', '3.25', '3.26', '3.8', '4.24', '4.28', '4.43', '4.44', '4.58', '4.61', '4.63', '4.72', '4.817']\n"
     ]
    }
   ],
   "source": [
    "#Find the conditional probability table and print, with labels. Note the guide after the table showing which fine-grained variable values have been coarsened together. \n",
    "P_CE = np.array([np.bincount(Y_lbls_prag.astype(int)[X_lbls_prag==X_lbl],minlength=Y_lbls_prag.max()+1).astype(float) for X_lbl in np.sort(np.unique(X_lbls_prag))])\n",
    "P_CE = P_CE/P_CE.sum()\n",
    "P_E_given_C = np.around(P_CE/P_CE.sum(axis=1, keepdims=True),3)\n",
    "column = np.array(['crs_C0','crs_C1','crs_C2']).reshape(-1,1)\n",
    "header = np.array(['    ','crs_E0','crs_E1','crs_E2'])\n",
    "P_E_given_C = np.hstack((column,P_E_given_C))\n",
    "P_E_given_C = np.vstack((header,P_E_given_C))\n",
    "print(P_E_given_C)\n",
    "print('crs_C0=', crs_C0)\n",
    "print('crs_C1=', crs_C1)\n",
    "print('crs_C2=', crs_C2)\n",
    "\n",
    "print('crs_E0=', crs_E0)\n",
    "print('crs_E1=', crs_E1)\n",
    "print('crs_E2=', crs_E2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ac9a6347239aa3fcc661fb868bad0c69b074a32b6ef5dc35de81e235dc1acb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
