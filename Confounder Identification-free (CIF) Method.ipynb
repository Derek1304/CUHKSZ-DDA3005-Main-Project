{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "class CIFCF():\n",
    "    def __init__(self, input_size, output_size, hidden_layers_num):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.structure = [self.input_size] + hidden_layers_num * [10] + [self.output_size]\n",
    "        self.W = []\n",
    "        self.B = []\n",
    "        self.All_Layers = []\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        for i in range(len(self.structure)-1):\n",
    "            self.W.append(torch.randn(self.structure[i+1], self.structure[i], requires_grad = True))\n",
    "            self.B.append(torch.randn(self.structure[i+1], requires_grad = True))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.All_Layers = []\n",
    "        self.input = x\n",
    "        self.All_Layers.append(x)\n",
    "        for i in range(len(self.structure)-2):\n",
    "            self.All_Layers.append(torch.relu(self.W[i] @ self.All_Layers[i] + self.B[i]))\n",
    "        self.output = torch.sigmoid(self.W[-1] @ self.All_Layers[-1] + self.B[-1])\n",
    "        return self.output\n",
    "    \n",
    "    def global_intervention(self, batch_X, batch_y):\n",
    "        global_intervention_W = []\n",
    "        global_intervention_B = []\n",
    "        for w in self.W:\n",
    "            global_intervention_W.append(torch.zeros_like(w))\n",
    "        for b in self.B:\n",
    "            global_intervention_B.append(torch.zeros_like(b))\n",
    "        \n",
    "        for i in range(batch_y.size(dim = 0)):\n",
    "            x = batch_X[i]\n",
    "            y = self.forward(x)\n",
    "            l = torch.nn.functional.binary_cross_entropy_with_logits(y ,batch_y[i])\n",
    "            l.backward(retain_graph = True)\n",
    "            for i in range(len(self.W)):\n",
    "                global_intervention_W[i] += self.W[i].grad\n",
    "                global_intervention_B[i] += self.B[i].grad\n",
    "        \n",
    "        return global_intervention_W, global_intervention_B\n",
    "    \n",
    "    def forward_with_global_intervention(self, x, global_intervention, alpha):\n",
    "        layers = []\n",
    "        layers.append(x)\n",
    "        for i in range(len(self.structure)-2):\n",
    "            layers.append(torch.relu((self.W[i] + alpha * global_intervention[0][i]) \n",
    "                                    @ layers[i] + self.B[i] + alpha * global_intervention[1][i]))\n",
    "        self.output = torch.sigmoid((self.W[-1] + alpha * global_intervention[0][-1]) \n",
    "                                    @ self.All_Layers[-1] + self.B[-1] + alpha * global_intervention[1][-1])\n",
    "        return self.output\n",
    "    \n",
    "    def update_model(self, batch_X, batch_y, alpha, beta, X, y, epoch):\n",
    "\n",
    "        copy_W = self.W\n",
    "        copy_B = self.B\n",
    "        M = batch_y.size(dim = 0)\n",
    "\n",
    "        global_intervention = self.global_intervention(batch_X, batch_y)\n",
    "        y_predicted = torch.empty_like(y)\n",
    "        for i in range(y.size(dim = 0)):\n",
    "            y_predicted[i] = self.forward_with_global_intervention(X[i], global_intervention, alpha/M)\n",
    "        \n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(y_predicted, y)\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i].grad = None\n",
    "            self.B[i].grad = None\n",
    "        loss.backward()\n",
    "        for i in range(len(self.W)):\n",
    "            tmp1 = self.W[i].grad\n",
    "            tmp2 = self.B[i].grad\n",
    "            with torch.no_grad():\n",
    "                self.W[i] = copy_W[i] - beta * tmp1\n",
    "                self.B[i] = copy_B[i] - beta * tmp2\n",
    "        print('Model updated, loss:' + str(loss.item()))\n",
    "        for i in range(len(self.W)):\n",
    "            self.W[i].requires_grad_(True)\n",
    "            self.B[i].requires_grad_(True)\n",
    "\n",
    "\n",
    "# model = CIFCF(4, 2, 1)\n",
    "# print(model.structure)\n",
    "# print(model.forward(torch.ones(4)).data)\n",
    "# print(model.W[0][0].size(dim=0))\n",
    "# print(model.global_intervention((torch.ones(4), torch.ones(4))))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "def load_dataset():\n",
    "    train_data = pd.read_csv('CN_data.csv')\n",
    "    feature_names = ['Value', 'UNRATE']\n",
    "    X = np.array(train_data[feature_names])\n",
    "    # X = preprocessing.normalize(X)\n",
    "    # 5000e8, 180000e8 \n",
    "    y = np.array(train_data['GDP'])\n",
    "    y_tmp = []\n",
    "    for GDP in y:\n",
    "        if GDP <= 1e12:\n",
    "            y_tmp.append([1] + [0]*17)\n",
    "        elif GDP <= 2e12:\n",
    "            y_tmp.append([0]*1 + [1] + [0]*16)\n",
    "        elif GDP <= 3e12:\n",
    "            y_tmp.append([0]*2 + [1] + [0]*15)\n",
    "        elif GDP <= 4e12:\n",
    "            y_tmp.append([0]*3 + [1] + [0]*14)\n",
    "        elif GDP <= 5e12:\n",
    "            y_tmp.append([0]*4 + [1] + [0]*13)\n",
    "        elif GDP <= 6e12:\n",
    "            y_tmp.append([0]*5 + [1] + [0]*12)\n",
    "        elif GDP <= 7e12:\n",
    "            y_tmp.append([0]*6 + [1] + [0]*11)\n",
    "        elif GDP <= 8e12:\n",
    "            y_tmp.append([0]*7 + [1] + [0]*10)\n",
    "        elif GDP <= 9e12:\n",
    "            y_tmp.append([0]*8 + [1] + [0]*9)\n",
    "        elif GDP <= 10e12:\n",
    "            y_tmp.append([0]*9 + [1] + [0]*8)\n",
    "        elif GDP <= 11e12:\n",
    "            y_tmp.append([0]*10 + [1] + [0]*7)\n",
    "        elif GDP <= 12e12:\n",
    "            y_tmp.append([0]*11 + [1] + [0]*6)\n",
    "        elif GDP <= 13e12:\n",
    "            y_tmp.append([0]*12 + [1] + [0]*5)\n",
    "        elif GDP <= 14e12:\n",
    "            y_tmp.append([0]*13 + [1] + [0]*4)\n",
    "        elif GDP <= 15e12:\n",
    "            y_tmp.append([0]*14 + [1] + [0]*3)\n",
    "        elif GDP <= 16e12:\n",
    "            y_tmp.append([0]*15 + [1] + [0]*2)\n",
    "        elif GDP <= 17e12:\n",
    "            y_tmp.append([0]*16 + [1] + [0]*1)\n",
    "        else:\n",
    "            y_tmp.append([0]*17 + [1])\n",
    "    y = np.array(y_tmp)\n",
    "    return X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(X, y, alpha, beta, K, batch_size, num_epochs):\n",
    "    # build a model\n",
    "    model = CIFCF(X.size(dim=1), y.size(dim=1), 2)\n",
    "    \n",
    "    # do cluster\n",
    "    kmeans = KMeans(n_clusters=K, random_state=0)\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        cluster_indices = [torch.where(torch.Tensor(cluster_labels == k))[0] for k in range(K)]\n",
    "        batch_X = []\n",
    "        batch_y = []\n",
    "        for k in range(K):\n",
    "            batch_indices = torch.randint(low=0, high=len(cluster_indices[k]), size=(batch_size,))\n",
    "            batch_X.append(X[cluster_indices[k][batch_indices]].numpy())\n",
    "            batch_y.append(y[cluster_indices[k][batch_indices]].numpy())\n",
    "        batch_X = np.array(batch_X)\n",
    "        batch_X = np.reshape(batch_X, (batch_X.shape[0] * batch_X.shape[1], batch_X.shape[2]))\n",
    "        batch_X = torch.from_numpy(batch_X)\n",
    "        batch_y = np.array(batch_y)\n",
    "        batch_y = np.reshape(batch_y, (batch_y.shape[0] * batch_y.shape[1], batch_y.shape[2]))\n",
    "        batch_y = torch.from_numpy(batch_y)\n",
    "        \n",
    "        model.update_model(batch_X, batch_y, alpha, beta, X, y, epoch)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model updated, loss:0.9299614429473877\n",
      "Model updated, loss:0.9256110191345215\n",
      "Model updated, loss:0.9792195558547974\n",
      "Model updated, loss:0.9260925054550171\n",
      "Model updated, loss:0.9257679581642151\n",
      "Model updated, loss:1.0241601467132568\n",
      "Model updated, loss:0.9262093901634216\n",
      "Model updated, loss:0.9253810048103333\n",
      "Model updated, loss:0.9978370666503906\n",
      "Model updated, loss:0.9294636249542236\n",
      "Model updated, loss:0.9152834415435791\n",
      "Model updated, loss:0.9313403964042664\n",
      "Model updated, loss:0.9763050675392151\n",
      "Model updated, loss:0.922038733959198\n",
      "Model updated, loss:0.9714675545692444\n",
      "Model updated, loss:0.9230933785438538\n",
      "Model updated, loss:0.922995924949646\n",
      "Model updated, loss:0.9379460215568542\n",
      "Model updated, loss:1.025076150894165\n",
      "Model updated, loss:0.9154303669929504\n"
     ]
    }
   ],
   "source": [
    "X, y = load_dataset()\n",
    "X = torch.Tensor(X)\n",
    "y = torch.Tensor(y)\n",
    "alpha = 0.01\n",
    "beta = 0.01\n",
    "K = 3\n",
    "batch_size = 5\n",
    "num_epochs = 20\n",
    "main(X, y, alpha, beta, K, batch_size, num_epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ac9a6347239aa3fcc661fb868bad0c69b074a32b6ef5dc35de81e235dc1acb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
